1. 
 Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], value function: [ 49.66895295  55.27161487  61.57363146  65.87460207  48.00953133
  52.30188839  68.13923599  73.25481858  50.22617115  -0.42481753
  77.06611566  81.36344171  66.36179067  76.31383816 100.
  89.90583635   0.        ], #iteration: 26
2. 
Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], value function: [ 49.69078867  55.28617892  61.58230087  65.87897994  48.03187576
  52.32047965  68.1447605   73.25676304  50.23031164  -0.41942079
  77.06767431  81.36397885  66.36430029  76.31513999 100.
  89.90596733   0.        ], #iteration: 7
3. 
#EvalIterations: 1, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 26
#EvalIterations: 2, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 15
#EvalIterations: 3, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 11
#EvalIterations: 4, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 9
#EvalIterations: 5, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 9
#EvalIterations: 6, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 8
#EvalIterations: 7, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 7
#EvalIterations: 8, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 8
#EvalIterations: 9, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 8
#EvalIterations: 10, Policy: [3 3 1 1 3 0 1 1 1 3 3 1 3 3 0 2 0], #iteration: 8
4. 
Initially, the higher the number of partial policy evaluations, the less iterations are needed for modified policy iteration. 
When the number of partial policy iteration reaches a point where it converges internally, it's equivalent to policy iterations. 
Value iteration needs more iterations to converge, while policy iteration converges faster, at some point the value might take larger steps due to direct evaluation. 
However in genral policy iteration converges faster than value iterations.